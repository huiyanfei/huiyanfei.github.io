I"xŒ<p>##å‰è¨€
ESIMï¼Œç®€ç§° <code class="language-plaintext highlighter-rouge">"Enhanced LSTM for Natural Language Inference"</code>.</p>

<p>é¡¾åæ€ä¹‰ï¼Œä¸€ç§ä¸“ä¸ºè‡ªç„¶è¯­è¨€æ¨æ–­è€Œç”Ÿçš„åŠ å¼ºç‰ˆ LSTM.</p>

<p>ESIM èƒ½æ¯”å…¶ä»–çŸ­æ–‡æœ¬åˆ†ç±»ç®—æ³•ç‰›é€¼ä¸»è¦åœ¨äºä¸¤ç‚¹ï¼š</p>

<ul>
  <li>
    <p><code class="language-plaintext highlighter-rouge">1.ç²¾ç»†çš„è®¾è®¡åºåˆ—å¼çš„æ¨æ–­ç»“æ„.</code></p>
  </li>
  <li>
    <p><code class="language-plaintext highlighter-rouge">2.è€ƒè™‘å±€éƒ¨æ¨æ–­å’Œå…¨å±€æ¨æ–­.</code></p>
  </li>
</ul>

<p>##1.æ¨¡å‹ç»“æ„
ESIMçš„è®ºæ–‡ä¸­ï¼Œä½œè€…æå‡ºäº†ä¸¤ç§ç»“æ„ï¼Œå¦‚ä¸‹å›¾æ‰€ç¤ºï¼Œå·¦è¾¹æ˜¯è‡ªç„¶è¯­è¨€ç†è§£æ¨¡å‹ESIMï¼Œå³è¾¹æ˜¯åŸºäºè¯­æ³•æ ‘ç»“æ„çš„HIM. æœ¬æ–‡ä¹Ÿä¸»è¦è®²è§£ESIMçš„ç»“æ„ï¼Œå¦‚æœå¯¹HIMæ„Ÿå…´è¶£çš„è¯å¯ä»¥é˜…è¯»åŸè®ºæ–‡ã€‚</p>

<p><img src="../pictures/ESIM/ESIM1.png" alt="avatar" /></p>

<p>##</p>

<p>ESIMä¸€å…±åŒ…å«å››éƒ¨åˆ†ï¼šInput Encodingã€Local Inference Modelingã€ Inference Compositionã€Predictionï¼Œæ¥ä¸‹æ¥ä¼šåˆ†åˆ«å¯¹è¿™å››éƒ¨åˆ†è¿›è¡Œè®²è§£ã€‚
##
<img src="../pictures/ESIM/ESIM2.png" alt="avatar" /></p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="nb">input</span><span class="p">):</span>
   <span class="c1"># batch_size * seq_len
</span>    <span class="n">sent1</span><span class="p">,</span> <span class="n">sent2</span> <span class="o">=</span> <span class="nb">input</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="nb">input</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
    <span class="n">mask1</span><span class="p">,</span> <span class="n">mask2</span> <span class="o">=</span> <span class="n">sent1</span><span class="p">.</span><span class="n">eq</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span> <span class="n">sent2</span><span class="p">.</span><span class="n">eq</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>

   <span class="c1"># embeds: batch_size * seq_len =&gt; batch_size * seq_len * embeds_dim
</span>    <span class="n">x1</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">bn_embeds</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">embeds</span><span class="p">(</span><span class="n">sent1</span><span class="p">).</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">).</span><span class="n">contiguous</span><span class="p">()).</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
    <span class="n">x2</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">bn_embeds</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">embeds</span><span class="p">(</span><span class="n">sent2</span><span class="p">).</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">).</span><span class="n">contiguous</span><span class="p">()).</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>

   <span class="c1"># batch_size * seq_len * embeds_dim =&gt; batch_size * seq_len * hidden_size
</span>    <span class="n">o1</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">lstm1</span><span class="p">(</span><span class="n">x1</span><span class="p">)</span>
    <span class="n">o2</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">lstm1</span><span class="p">(</span><span class="n">x2</span><span class="p">)</span>    
</code></pre></div></div>
<p>##
<img src="../pictures/ESIM/ESIM3.png" alt="avatar" /></p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">soft_align_attention</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">,</span> <span class="n">mask1</span><span class="p">,</span> <span class="n">mask2</span><span class="p">):</span>
    <span class="s">'''
     x1: batch_size * seq_len * hidden_size
     x2: batch_size * seq_len * hidden_size
    '''</span>
    <span class="c1"># attention: batch_size * seq_len * seq_len
</span>     <span class="n">attention</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span>
     <span class="n">mask1</span> <span class="o">=</span> <span class="n">mask1</span><span class="p">.</span><span class="nb">float</span><span class="p">().</span><span class="n">masked_fill_</span><span class="p">(</span><span class="n">mask1</span><span class="p">,</span> <span class="nb">float</span><span class="p">(</span><span class="s">'-inf'</span><span class="p">))</span>
     <span class="n">mask2</span> <span class="o">=</span> <span class="n">mask2</span><span class="p">.</span><span class="nb">float</span><span class="p">().</span><span class="n">masked_fill_</span><span class="p">(</span><span class="n">mask2</span><span class="p">,</span> <span class="nb">float</span><span class="p">(</span><span class="s">'-inf'</span><span class="p">))</span>

    <span class="c1"># weight: batch_size * seq_len * seq_len
</span>     <span class="n">weight1</span> <span class="o">=</span> <span class="n">F</span><span class="p">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">attention</span> <span class="o">+</span> <span class="n">mask2</span><span class="p">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">),</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
     <span class="n">x1_align</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">weight1</span><span class="p">,</span> <span class="n">x2</span><span class="p">)</span>
     <span class="n">weight2</span> <span class="o">=</span> <span class="n">F</span><span class="p">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">attention</span><span class="p">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span> <span class="o">+</span> <span class="n">mask1</span><span class="p">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">),</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
     <span class="n">x2_align</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">weight2</span><span class="p">,</span> <span class="n">x1</span><span class="p">)</span>
   
    <span class="c1"># x_align: batch_size * seq_len * hidden_size
</span>     <span class="k">return</span> <span class="n">x1_align</span><span class="p">,</span> <span class="n">x2_align</span>    

<span class="k">def</span> <span class="nf">submul</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">):</span>
    <span class="n">mul</span> <span class="o">=</span> <span class="n">x1</span> <span class="o">*</span> <span class="n">x2</span>
    <span class="n">sub</span> <span class="o">=</span> <span class="n">x1</span> <span class="o">-</span> <span class="n">x2</span>
    <span class="k">return</span> <span class="n">torch</span><span class="p">.</span><span class="n">cat</span><span class="p">([</span><span class="n">sub</span><span class="p">,</span> <span class="n">mul</span><span class="p">],</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>    

<span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="nb">input</span><span class="p">):</span>
    <span class="err">Â·Â·Â·</span>
    
    <span class="c1"># Attention
</span>    <span class="c1"># output: batch_size * seq_len * hidden_size
</span>    <span class="n">q1_align</span><span class="p">,</span> <span class="n">q2_align</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">soft_align_attention</span><span class="p">(</span><span class="n">o1</span><span class="p">,</span> <span class="n">o2</span><span class="p">,</span> <span class="n">mask1</span><span class="p">,</span> <span class="n">mask2</span><span class="p">)</span>

    <span class="c1"># Enhancement of local inference information
</span>    <span class="c1"># batch_size * seq_len * (8 * hidden_size)
</span>    <span class="n">q1_combined</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">cat</span><span class="p">([</span><span class="n">o1</span><span class="p">,</span> <span class="n">q1_align</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">submul</span><span class="p">(</span><span class="n">o1</span><span class="p">,</span> <span class="n">q1_align</span><span class="p">)],</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">q2_combined</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">cat</span><span class="p">([</span><span class="n">o2</span><span class="p">,</span> <span class="n">q2_align</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">submul</span><span class="p">(</span><span class="n">o2</span><span class="p">,</span> <span class="n">q2_align</span><span class="p">)],</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>

    <span class="p">...</span>
</code></pre></div></div>
<p>##
<img src="../pictures/ESIM/ESIM4.png" alt="avatar" /></p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">apply_multiple</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
    <span class="c1"># input: batch_size * seq_len * (2 * hidden_size)
</span>    <span class="n">p1</span> <span class="o">=</span> <span class="n">F</span><span class="p">.</span><span class="n">avg_pool1d</span><span class="p">(</span><span class="n">x</span><span class="p">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="n">x</span><span class="p">.</span><span class="n">size</span><span class="p">(</span><span class="mi">1</span><span class="p">)).</span><span class="n">squeeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">p2</span> <span class="o">=</span> <span class="n">F</span><span class="p">.</span><span class="n">max_pool1d</span><span class="p">(</span><span class="n">x</span><span class="p">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="n">x</span><span class="p">.</span><span class="n">size</span><span class="p">(</span><span class="mi">1</span><span class="p">)).</span><span class="n">squeeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
    <span class="c1"># output: batch_size * (4 * hidden_size)
</span>    <span class="k">return</span> <span class="n">torch</span><span class="p">.</span><span class="n">cat</span><span class="p">([</span><span class="n">p1</span><span class="p">,</span> <span class="n">p2</span><span class="p">],</span> <span class="mi">1</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="nb">input</span><span class="p">):</span>
    <span class="p">...</span>
    
    <span class="c1"># inference composition
</span>    <span class="c1"># batch_size * seq_len * (2 * hidden_size)
</span>    <span class="n">q1_compose</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">lstm2</span><span class="p">(</span><span class="n">q1_combined</span><span class="p">)</span>
    <span class="n">q2_compose</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">lstm2</span><span class="p">(</span><span class="n">q2_combined</span><span class="p">)</span>

    <span class="c1"># Aggregate
</span>    <span class="c1"># input: batch_size * seq_len * (2 * hidden_size)
</span>    <span class="c1"># output: batch_size * (4 * hidden_size)
</span>    <span class="n">q1_rep</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">apply_multiple</span><span class="p">(</span><span class="n">q1_compose</span><span class="p">)</span>
    <span class="n">q2_rep</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">apply_multiple</span><span class="p">(</span><span class="n">q2_compose</span><span class="p">)</span>

    <span class="c1"># Classifier
</span>    <span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">cat</span><span class="p">([</span><span class="n">q1_rep</span><span class="p">,</span> <span class="n">q2_rep</span><span class="p">],</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">sim</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">fc</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">sim</span>
</code></pre></div></div>
<p>##
###å®Œæ•´ä»£ç ï¼š</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">torch</span> <span class="kn">import</span> <span class="n">nn</span>
<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.nn.functional</span> <span class="k">as</span> <span class="n">F</span>


<span class="k">class</span> <span class="nc">ESIM</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">args</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">ESIM</span><span class="p">,</span> <span class="bp">self</span><span class="p">).</span><span class="n">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">args</span> <span class="o">=</span> <span class="n">args</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="mf">0.5</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">hidden_size</span> <span class="o">=</span> <span class="n">args</span><span class="p">.</span><span class="n">hidden_size</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">embeds_dim</span> <span class="o">=</span> <span class="n">args</span><span class="p">.</span><span class="n">embeds_dim</span>
        <span class="n">num_word</span> <span class="o">=</span> <span class="mi">20000</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">embeds</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Embedding</span><span class="p">(</span><span class="n">num_word</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">embeds_dim</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">bn_embeds</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">BatchNorm1d</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">embeds_dim</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">lstm1</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">LSTM</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">embeds_dim</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">hidden_size</span><span class="p">,</span> <span class="n">batch_first</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">bidirectional</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">lstm2</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">LSTM</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">hidden_size</span><span class="o">*</span><span class="mi">8</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">hidden_size</span><span class="p">,</span> <span class="n">batch_first</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">bidirectional</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

        <span class="bp">self</span><span class="p">.</span><span class="n">fc</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Sequential</span><span class="p">(</span>
            <span class="n">nn</span><span class="p">.</span><span class="n">BatchNorm1d</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">hidden_size</span> <span class="o">*</span> <span class="mi">8</span><span class="p">),</span>
            <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">hidden_size</span> <span class="o">*</span> <span class="mi">8</span><span class="p">,</span> <span class="n">args</span><span class="p">.</span><span class="n">linear_size</span><span class="p">),</span>
            <span class="n">nn</span><span class="p">.</span><span class="n">ELU</span><span class="p">(</span><span class="n">inplace</span><span class="o">=</span><span class="bp">True</span><span class="p">),</span>
            <span class="n">nn</span><span class="p">.</span><span class="n">BatchNorm1d</span><span class="p">(</span><span class="n">args</span><span class="p">.</span><span class="n">linear_size</span><span class="p">),</span>
            <span class="n">nn</span><span class="p">.</span><span class="n">Dropout</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">dropout</span><span class="p">),</span>
            <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">args</span><span class="p">.</span><span class="n">linear_size</span><span class="p">,</span> <span class="n">args</span><span class="p">.</span><span class="n">linear_size</span><span class="p">),</span>
            <span class="n">nn</span><span class="p">.</span><span class="n">ELU</span><span class="p">(</span><span class="n">inplace</span><span class="o">=</span><span class="bp">True</span><span class="p">),</span>
            <span class="n">nn</span><span class="p">.</span><span class="n">BatchNorm1d</span><span class="p">(</span><span class="n">args</span><span class="p">.</span><span class="n">linear_size</span><span class="p">),</span>
            <span class="n">nn</span><span class="p">.</span><span class="n">Dropout</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">dropout</span><span class="p">),</span>
            <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">args</span><span class="p">.</span><span class="n">linear_size</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span>
            <span class="n">nn</span><span class="p">.</span><span class="n">Softmax</span><span class="p">(</span><span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
        <span class="p">)</span>
    
    <span class="k">def</span> <span class="nf">soft_attention_align</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">,</span> <span class="n">mask1</span><span class="p">,</span> <span class="n">mask2</span><span class="p">):</span>
        <span class="s">'''
        x1: batch_size * seq_len * dim
        x2: batch_size * seq_len * dim
        '''</span>
        <span class="c1"># attention: batch_size * seq_len * seq_len
</span>        <span class="n">attention</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span>
        <span class="n">mask1</span> <span class="o">=</span> <span class="n">mask1</span><span class="p">.</span><span class="nb">float</span><span class="p">().</span><span class="n">masked_fill_</span><span class="p">(</span><span class="n">mask1</span><span class="p">,</span> <span class="nb">float</span><span class="p">(</span><span class="s">'-inf'</span><span class="p">))</span>
        <span class="n">mask2</span> <span class="o">=</span> <span class="n">mask2</span><span class="p">.</span><span class="nb">float</span><span class="p">().</span><span class="n">masked_fill_</span><span class="p">(</span><span class="n">mask2</span><span class="p">,</span> <span class="nb">float</span><span class="p">(</span><span class="s">'-inf'</span><span class="p">))</span>

        <span class="c1"># weight: batch_size * seq_len * seq_len
</span>        <span class="n">weight1</span> <span class="o">=</span> <span class="n">F</span><span class="p">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">attention</span> <span class="o">+</span> <span class="n">mask2</span><span class="p">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">),</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">x1_align</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">weight1</span><span class="p">,</span> <span class="n">x2</span><span class="p">)</span>
        <span class="n">weight2</span> <span class="o">=</span> <span class="n">F</span><span class="p">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">attention</span><span class="p">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span> <span class="o">+</span> <span class="n">mask1</span><span class="p">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">),</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">x2_align</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">weight2</span><span class="p">,</span> <span class="n">x1</span><span class="p">)</span>
        <span class="c1"># x_align: batch_size * seq_len * hidden_size
</span>
        <span class="k">return</span> <span class="n">x1_align</span><span class="p">,</span> <span class="n">x2_align</span>

    <span class="k">def</span> <span class="nf">submul</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">):</span>
        <span class="n">mul</span> <span class="o">=</span> <span class="n">x1</span> <span class="o">*</span> <span class="n">x2</span>
        <span class="n">sub</span> <span class="o">=</span> <span class="n">x1</span> <span class="o">-</span> <span class="n">x2</span>
        <span class="k">return</span> <span class="n">torch</span><span class="p">.</span><span class="n">cat</span><span class="p">([</span><span class="n">sub</span><span class="p">,</span> <span class="n">mul</span><span class="p">],</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">apply_multiple</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="c1"># input: batch_size * seq_len * (2 * hidden_size)
</span>        <span class="n">p1</span> <span class="o">=</span> <span class="n">F</span><span class="p">.</span><span class="n">avg_pool1d</span><span class="p">(</span><span class="n">x</span><span class="p">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="n">x</span><span class="p">.</span><span class="n">size</span><span class="p">(</span><span class="mi">1</span><span class="p">)).</span><span class="n">squeeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">p2</span> <span class="o">=</span> <span class="n">F</span><span class="p">.</span><span class="n">max_pool1d</span><span class="p">(</span><span class="n">x</span><span class="p">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="n">x</span><span class="p">.</span><span class="n">size</span><span class="p">(</span><span class="mi">1</span><span class="p">)).</span><span class="n">squeeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
        <span class="c1"># output: batch_size * (4 * hidden_size)
</span>        <span class="k">return</span> <span class="n">torch</span><span class="p">.</span><span class="n">cat</span><span class="p">([</span><span class="n">p1</span><span class="p">,</span> <span class="n">p2</span><span class="p">],</span> <span class="mi">1</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="nb">input</span><span class="p">):</span>
        <span class="c1"># batch_size * seq_len
</span>        <span class="n">sent1</span><span class="p">,</span> <span class="n">sent2</span> <span class="o">=</span> <span class="nb">input</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="nb">input</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
        <span class="n">mask1</span><span class="p">,</span> <span class="n">mask2</span> <span class="o">=</span> <span class="n">sent1</span><span class="p">.</span><span class="n">eq</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span> <span class="n">sent2</span><span class="p">.</span><span class="n">eq</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>

        <span class="c1"># embeds: batch_size * seq_len =&gt; batch_size * seq_len * dim
</span>        <span class="n">x1</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">bn_embeds</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">embeds</span><span class="p">(</span><span class="n">sent1</span><span class="p">).</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">).</span><span class="n">contiguous</span><span class="p">()).</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
        <span class="n">x2</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">bn_embeds</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">embeds</span><span class="p">(</span><span class="n">sent2</span><span class="p">).</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">).</span><span class="n">contiguous</span><span class="p">()).</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>

        <span class="c1"># batch_size * seq_len * dim =&gt; batch_size * seq_len * hidden_size
</span>        <span class="n">o1</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">lstm1</span><span class="p">(</span><span class="n">x1</span><span class="p">)</span>
        <span class="n">o2</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">lstm1</span><span class="p">(</span><span class="n">x2</span><span class="p">)</span>

        <span class="c1"># Attention
</span>        <span class="c1"># batch_size * seq_len * hidden_size
</span>        <span class="n">q1_align</span><span class="p">,</span> <span class="n">q2_align</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">soft_attention_align</span><span class="p">(</span><span class="n">o1</span><span class="p">,</span> <span class="n">o2</span><span class="p">,</span> <span class="n">mask1</span><span class="p">,</span> <span class="n">mask2</span><span class="p">)</span>
        
        <span class="c1"># Compose
</span>        <span class="c1"># batch_size * seq_len * (8 * hidden_size)
</span>        <span class="n">q1_combined</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">cat</span><span class="p">([</span><span class="n">o1</span><span class="p">,</span> <span class="n">q1_align</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">submul</span><span class="p">(</span><span class="n">o1</span><span class="p">,</span> <span class="n">q1_align</span><span class="p">)],</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">q2_combined</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">cat</span><span class="p">([</span><span class="n">o2</span><span class="p">,</span> <span class="n">q2_align</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">submul</span><span class="p">(</span><span class="n">o2</span><span class="p">,</span> <span class="n">q2_align</span><span class="p">)],</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>

        <span class="c1"># batch_size * seq_len * (2 * hidden_size)
</span>        <span class="n">q1_compose</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">lstm2</span><span class="p">(</span><span class="n">q1_combined</span><span class="p">)</span>
        <span class="n">q2_compose</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">lstm2</span><span class="p">(</span><span class="n">q2_combined</span><span class="p">)</span>

        <span class="c1"># Aggregate
</span>        <span class="c1"># input: batch_size * seq_len * (2 * hidden_size)
</span>        <span class="c1"># output: batch_size * (4 * hidden_size)
</span>        <span class="n">q1_rep</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">apply_multiple</span><span class="p">(</span><span class="n">q1_compose</span><span class="p">)</span>
        <span class="n">q2_rep</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">apply_multiple</span><span class="p">(</span><span class="n">q2_compose</span><span class="p">)</span>

        <span class="c1"># Classifier
</span>        <span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">cat</span><span class="p">([</span><span class="n">q1_rep</span><span class="p">,</span> <span class="n">q2_rep</span><span class="p">],</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">similarity</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">fc</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">similarity</span>

</code></pre></div></div>

:ET